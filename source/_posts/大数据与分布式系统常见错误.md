---
title: 大数据与分布式系统常见错误
date: 2021-10-15 21:48:43
index_img: /img/Apache_Spark_logo.svg.png
banner_img: /img/hadoop-vs-spark-main-1280x720.jpeg
tags: [Spark,大数据,conda]
---

> 本文为MSc Data Project 开发过程中出现的问题汇总

<!-- more -->

### 运行Spark时显示`LOG`目录没有更改权限

> chown: changing ownership of ‘/data/opt/msc_big_data/spark-3.0.1-bin-hadoop2.7/logs’: Operation not permitted

由于使用的为实验室集群，MSc学生用户没有`sudo`权限，而默认的Spark Log目录在其它用户文件夹中，所以我们需要更改`.bashrc`文件，增加对自己Spark Log目录的定义。

在`.bashrc`文件中添加：

```sh
export SPARK_LOG_DIR=/home/[user dir]/spark_log
```

> - 上述`[user dir]`需要更改为自己具体的位置
> - 集群中不止是 Master 机器，所有 Worker 机器都需要更改`.bashrc`

### 运行`etl.py`时出现`OutOfMemoryError: Java heap space`或者`GC overhead limited`

> ETL, which stands for extract, transform and load.

上述问题均是由于 spark-submit 运行时`driver-memory`参数设置过小导致的，只需要在运行 spark-submit时添加 `--driver-memory 10g`参数便可，此处我设置参数大小为10g，视具体情况改变。运行命令示例：

```sh
spark-submit --driver-memory 10g etl.py
```

### 运行`etl.py`时出现 Master 机器与 Worker 机器 Python 版本不同的问题

> exception: python in worker has different version 3.6 than that in driver 3.7, pyspark cannot run with different minor versions. please check environment variables pyspark_python and pyspark_driver_python are correctly set.

同样，由于使用的为实验室集群，MSc 学生用户没有`sudo`权限，更改不了 Worker 机器的 Python 版本，于是我们只能曲线救国使用`miniconda`创建虚拟环境来更改 Worker 机器的 Python版本。

#### 下载`miniconda`安装脚本

在 miniconda 官网[下载页面](https://docs.conda.io/en/latest/miniconda.html)中我们可以找到对应 Python 版本的安装脚本：

![Xnip2021-09-29_21-38-47](/img/Xnip2021-09-29_21-38-47.png)

右键复制下载链接，在 Worker 机器中输入`wget [下载链接]`便可以获取安装脚本，运行命令示例：

```sh
wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.10.3-Linux-x86_64.sh
```

#### 安装`miniconda`

运行安装脚本，遵循提示安装：

```sh
bash Miniconda3-py37_4.10.3-Linux-x86_64.sh
```

#### 创建虚拟环境，设置为默认环境

创建虚拟环境：

```sh
conda create --name [your preferred name] python=3.7
```

添加以下激活命令到`.bashrc`文件中:

```sh
activate [your preferred name]
```

之后使其生效：

```sh
resource .bashrc
```

### 运行Spark之后出现WebUI端口占用问题

```sh
ERROR ui.MasterWebUI: Failed to bind MasterWebUI
java.net.BindException: Failed to bind to /0.0.0.0:8096: Service 'MasterUI' failed after 16 retries (starting from 8080)!
```

出现上述问题说明端口`8080`~`8096`都已经被占用，而此时我们没有sudo权限，更改不了Spark里面`start-all.sh`的内容，则需要更改`spark-env.sh`，在里面添加:

```sh
export SPARK_MASTER_WEBUI_PORT=13210 #set port no. to your preferred no.
```

